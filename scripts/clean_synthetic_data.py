#!/usr/bin/env python3
"""
Clean synthetic IMDB reviews generated by Qwen3.

Removes meta-commentary, truncated content, and validates review quality.
"""

import json
import re
import argparse
from pathlib import Path
from typing import List, Dict, Tuple
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ReviewCleaner:
    """Clean and validate synthetic movie reviews."""

    # Patterns that indicate meta-commentary
    META_PATTERNS = [
        r'\n+(Okay|Alright|Wait|Actually|Let me|First|Now|Next|So|The user|I need to|I should|I can|I think|Here\'s|Another review).{0,500}$',
        r'^(Okay|Alright|Wait|Actually)[\s,].{0,100}',  # Starting with meta words
        r'\n+\[.*?\].*$',  # [DEBUG] or [NOTE] style comments
        r'\n+\*.*?\*.*$',  # Markdown-style notes
        r'The review (text |you\'ve requested |is:|should be:|needs to be)',
        r'(I\'m sorry|I apologize|I cannot provide)',
        r'(draft|version|example|template|placeholder|fictional)',
        r'\n+Review\s*$',  # Trailing "Review" marker
        r'\.\s*\n+[A-Z].*?(review|movie|film).*?:',  # Review labels
    ]

    # Indicators of incomplete/truncated reviews
    TRUNCATION_PATTERNS = [
        r'\.\.\.$',  # Ends with ellipsis
        r'[^.!?]\s*$',  # Doesn't end with punctuation
    ]

    # Minimum quality thresholds
    MIN_LENGTH = 50  # Minimum characters
    MAX_LENGTH = 1000  # Maximum characters (real reviews are usually shorter)
    MIN_SENTENCES = 2  # At least 2 sentences

    def __init__(self):
        self.meta_regex = [re.compile(pattern, re.IGNORECASE | re.DOTALL) for pattern in self.META_PATTERNS]
        self.truncation_regex = [re.compile(pattern) for pattern in self.TRUNCATION_PATTERNS]

    def remove_meta_commentary(self, text: str) -> str:
        """Remove meta-commentary from review text."""
        cleaned = text

        # First, split on common meta-commentary boundaries
        # Take only the first part before meta-commentary starts
        split_markers = [
            '\nOkay,', '\nAlright,', '\nWait,', '\nActually,',
            '\nLet me', '\nFirst,', '\nNow,', '\nAnother review',
            '\nYet another', '\nThe user', '\nis written in',
            '\nThe review is', '\nThis review', '"\nis written'
        ]
        for marker in split_markers:
            if marker in cleaned:
                cleaned = cleaned.split(marker)[0]

        # Apply each meta pattern
        for regex in self.meta_regex:
            cleaned = regex.sub('', cleaned)

        # Remove sentences that are obviously meta-commentary
        sentences = cleaned.split('. ')
        filtered_sentences = []
        for sent in sentences:
            sent_lower = sent.lower().strip()
            # Skip obvious meta sentences
            if any(phrase in sent_lower for phrase in [
                'let me start', 'let me try', 'let me think',
                'i need to', 'i should', 'i can help',
                'the user wants', 'the example', 'the query',
                'here\'s an', 'here is an'
            ]):
                continue
            filtered_sentences.append(sent)

        # Remove duplicate sentences (common in generated text)
        seen = set()
        unique_sentences = []
        for sent in filtered_sentences:
            sent_normalized = sent.lower().strip()
            if sent_normalized and sent_normalized not in seen and len(sent_normalized) > 10:
                seen.add(sent_normalized)
                unique_sentences.append(sent)

        cleaned = '. '.join(unique_sentences)

        # Ensure proper ending
        if cleaned and not cleaned[-1] in '.!?':
            # Find last sentence-ending punctuation
            last_punct = max(
                cleaned.rfind('.'),
                cleaned.rfind('!'),
                cleaned.rfind('?')
            )
            if last_punct > 0:
                cleaned = cleaned[:last_punct + 1]

        # Clean up whitespace
        cleaned = re.sub(r'\n\s*\n', '\n', cleaned)  # Multiple newlines -> single
        cleaned = re.sub(r' +', ' ', cleaned)  # Multiple spaces -> single
        cleaned = cleaned.strip()

        return cleaned

    def is_truncated(self, text: str) -> bool:
        """Check if text appears truncated."""
        for regex in self.truncation_regex:
            if regex.search(text):
                return True
        return False

    def count_sentences(self, text: str) -> int:
        """Count sentences in text."""
        # Simple sentence counter
        return len(re.findall(r'[.!?]+', text))

    def validate_review(self, text: str) -> Tuple[bool, str]:
        """
        Validate if cleaned text is a usable review.

        Returns:
            (is_valid, reason)
        """
        if not text:
            return False, "empty"

        if len(text) < self.MIN_LENGTH:
            return False, "too_short"

        if len(text) > self.MAX_LENGTH:
            return False, "too_long"

        if self.count_sentences(text) < self.MIN_SENTENCES:
            return False, "insufficient_sentences"

        # Check for common meta-commentary phrases that slipped through
        meta_phrases = [
            'let me', 'the user', 'i need to', 'i should',
            'okay so', 'alright so', 'wait let',
            'example:', 'template:', 'draft:',
            'okay, i just', 'okay i just',
        ]
        text_lower = text.lower()
        for phrase in meta_phrases:
            if phrase in text_lower:
                return False, "contains_meta"

        # Check if review starts with "Okay" or "Alright" (common meta pattern)
        if text_lower.startswith(('okay,', 'okay ', 'alright,', 'alright ')):
            return False, "meta_start"

        # Check for meta sentences about the review itself
        meta_review_phrases = [
            'is written in the style',
            'the review is',
            'this review',
            'as requested',
            'per your request'
        ]
        for phrase in meta_review_phrases:
            if phrase in text_lower:
                return False, "meta_about_review"

        return True, "valid"

    def clean_review(self, text: str) -> Tuple[str, bool, str]:
        """
        Clean a single review.

        Returns:
            (cleaned_text, is_valid, reason)
        """
        # Step 1: Remove meta-commentary
        cleaned = self.remove_meta_commentary(text)

        # Step 2: Validate
        is_valid, reason = self.validate_review(cleaned)

        return cleaned, is_valid, reason


def clean_dataset(input_file: Path, output_file: Path, class_name: str) -> Dict:
    """Clean a dataset file and save results."""
    logger.info(f"Loading {input_file}...")

    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    original_count = len(data['texts'])
    logger.info(f"Original samples: {original_count}")

    cleaner = ReviewCleaner()
    cleaned_texts = []
    stats = {
        'original': original_count,
        'valid': 0,
        'rejected': 0,
        'rejection_reasons': {}
    }

    # Process each review
    for idx, text in enumerate(data['texts']):
        cleaned, is_valid, reason = cleaner.clean_review(text)

        if is_valid:
            cleaned_texts.append(cleaned)
            stats['valid'] += 1
        else:
            stats['rejected'] += 1
            stats['rejection_reasons'][reason] = stats['rejection_reasons'].get(reason, 0) + 1

        # Progress indicator
        if (idx + 1) % 500 == 0:
            logger.info(f"Processed {idx + 1}/{original_count} ({stats['valid']} valid)")

    # Save cleaned data
    output_data = {
        'class_id': data['class_id'],
        'class_name': class_name,
        'num_examples': len(cleaned_texts),
        'texts': cleaned_texts,
        'cleaning_stats': stats
    }

    logger.info(f"Writing {len(cleaned_texts)} cleaned samples to {output_file}...")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)

    return stats


def main():
    parser = argparse.ArgumentParser(description='Clean synthetic IMDB review data')
    parser.add_argument('--input-dir', type=str, default='data/synthetic-imdb',
                        help='Input directory containing raw synthetic data')
    parser.add_argument('--output-dir', type=str, default='data/synthetic-imdb-cleaned',
                        help='Output directory for cleaned data')
    parser.add_argument('--remote-copy', action='store_true',
                        help='Copy data from nigel.birs.ca first')

    args = parser.parse_args()

    input_dir = Path(args.input_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Process both classes
    classes = [
        ('train_class_0.json', 'negative'),
        ('train_class_1.json', 'positive')
    ]

    total_stats = {
        'negative': None,
        'positive': None,
        'overall': {
            'original': 0,
            'valid': 0,
            'rejected': 0,
            'rejection_reasons': {}
        }
    }

    for filename, class_name in classes:
        input_file = input_dir / filename
        output_file = output_dir / filename

        if not input_file.exists():
            logger.error(f"Input file not found: {input_file}")
            continue

        logger.info(f"\n{'='*60}")
        logger.info(f"Processing {class_name.upper()} reviews")
        logger.info(f"{'='*60}")

        stats = clean_dataset(input_file, output_file, class_name)
        total_stats[class_name] = stats

        # Accumulate overall stats
        total_stats['overall']['original'] += stats['original']
        total_stats['overall']['valid'] += stats['valid']
        total_stats['overall']['rejected'] += stats['rejected']
        for reason, count in stats['rejection_reasons'].items():
            total_stats['overall']['rejection_reasons'][reason] = \
                total_stats['overall']['rejection_reasons'].get(reason, 0) + count

    # Save metadata
    metadata = {
        'dataset': 'synthetic_imdb_cleaned',
        'task': 'sentiment_classification',
        'domain': 'movie_reviews',
        'num_classes': 2,
        'class_names': ['negative', 'positive'],
        'synthetic': True,
        'generator': 'Qwen/Qwen3-8B',
        'cleaned': True,
        'splits': {
            'train': {
                'negative': total_stats['negative']['valid'] if total_stats['negative'] else 0,
                'positive': total_stats['positive']['valid'] if total_stats['positive'] else 0
            }
        },
        'cleaning_stats': total_stats
    }

    metadata_file = output_dir / 'metadata.json'
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    # Print summary
    logger.info(f"\n{'='*60}")
    logger.info("CLEANING SUMMARY")
    logger.info(f"{'='*60}")
    logger.info(f"Original samples: {total_stats['overall']['original']}")
    logger.info(f"Valid samples: {total_stats['overall']['valid']} ({total_stats['overall']['valid']/total_stats['overall']['original']*100:.1f}%)")
    logger.info(f"Rejected samples: {total_stats['overall']['rejected']} ({total_stats['overall']['rejected']/total_stats['overall']['original']*100:.1f}%)")
    logger.info(f"\nRejection reasons:")
    for reason, count in sorted(total_stats['overall']['rejection_reasons'].items(), key=lambda x: x[1], reverse=True):
        logger.info(f"  {reason}: {count} ({count/total_stats['overall']['rejected']*100:.1f}%)")
    logger.info(f"\nCleaned data saved to: {output_dir}")
    logger.info(f"Metadata saved to: {metadata_file}")


if __name__ == '__main__':
    main()
